{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98692592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba28059",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../1Classes/alchemist.txt',\"r\", encoding='utf-8') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d6d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(len(chars))\n",
    "char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "int2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "vocab = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533cbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.7493509909615237\n",
      "Epoch: 2, Loss: 1.442823504071539\n",
      "Epoch: 3, Loss: 1.3604286251732738\n",
      "Epoch: 4, Loss: 1.3164603889174953\n",
      "Epoch: 5, Loss: 1.2880956370630818\n",
      "Epoch: 6, Loss: 1.268713007774625\n",
      "Epoch: 7, Loss: 1.2536734792976525\n",
      "Epoch: 8, Loss: 1.2459016601203172\n",
      "Epoch: 9, Loss: 1.237612524586168\n",
      "Epoch: 10, Loss: 1.231329178474494\n",
      "Epoch: 11, Loss: 1.2280090127631034\n",
      "Epoch: 12, Loss: 1.2251963973418267\n",
      "Epoch: 13, Loss: 1.2281517317653905\n",
      "Epoch: 14, Loss: 1.2276080897544712\n",
      "Epoch: 15, Loss: 1.2257580647544808\n",
      "Epoch: 16, Loss: 1.2264776985637835\n",
      "Epoch: 17, Loss: 1.2288291255011508\n",
      "Epoch: 18, Loss: 1.2325118322483484\n",
      "Epoch: 19, Loss: 1.23379922816436\n",
      "Epoch: 20, Loss: 1.2371150639308257\n",
      "Epoch: 21, Loss: 1.238876079763022\n",
      "Epoch: 22, Loss: 1.2428622709034962\n",
      "Epoch: 23, Loss: 1.2458061728992933\n",
      "Epoch: 24, Loss: 1.2505912656248714\n",
      "Epoch: 25, Loss: 1.2558982329605601\n",
      "Epoch: 26, Loss: 1.2633340973840805\n",
      "Epoch: 27, Loss: 1.261546203834877\n",
      "Epoch: 28, Loss: 1.266388795713107\n",
      "Epoch: 29, Loss: 1.2695110383487882\n",
      "Epoch: 30, Loss: 1.2782183467492405\n",
      "Epoch: 31, Loss: 1.2826378834011407\n",
      "Epoch: 32, Loss: 1.2945999095205387\n",
      "Epoch: 33, Loss: 1.2913760488589356\n",
      "Epoch: 34, Loss: 1.295580586647051\n",
      "Epoch: 35, Loss: 1.300313324305926\n",
      "Epoch: 36, Loss: 1.3146810658615333\n",
      "Epoch: 37, Loss: 1.3129124396121623\n",
      "Epoch: 38, Loss: 1.318971327414395\n",
      "Epoch: 39, Loss: 1.3197980160550769\n",
      "Epoch: 40, Loss: 1.3223017946398419\n",
      "Epoch: 41, Loss: 1.3290251544005016\n",
      "Epoch: 42, Loss: 1.3298632473965506\n",
      "Epoch: 43, Loss: 1.339188388664357\n",
      "Epoch: 44, Loss: 1.3415995357473318\n",
      "Epoch: 45, Loss: 1.3462093540641804\n",
      "Epoch: 46, Loss: 1.3583020858562445\n",
      "Epoch: 47, Loss: 1.3605368878222357\n",
      "Epoch: 48, Loss: 1.3668446854704737\n",
      "Epoch: 49, Loss: 1.3719183620082285\n",
      "Epoch: 50, Loss: 1.3738442203590013\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "int2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "data_encoded = np.array([char2int[ch] for ch in text], dtype=np.int64)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, seq_length):\n",
    "    self.data = data\n",
    "    self.seq_length = seq_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.seq_length\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = self.data[idx: idx+self.seq_length]\n",
    "    y = self.data[idx+self.seq_length]\n",
    "    x = torch.tensor(x, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "split = int(len(data_encoded)*0.8)\n",
    "\n",
    "train_dataset = data_encoded[:split]\n",
    "test_dataset = data_encoded[split:]\n",
    "\n",
    "seq_len = 100\n",
    "batch_size = 64\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "train_dataset = TextDataset(train_dataset, seq_len)\n",
    "test_dataset = TextDataset(test_dataset, seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    emb = self.embedding(x) # [batch, seq] -> [batch, seq, embed_dim]\n",
    "    out, hidden = self.rnn(emb, hidden) # out: [batch, seq, hidden]\n",
    "    last = out[:, -1, :] # [batch, hidden]\n",
    "    logits = self.fc(last)\n",
    "    return logits, hidden\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CharRNN(vocab_size=vocab, embed_size=embed_size, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "  hidden = model.init_hidden(batch_size).to(device)\n",
    "  total_loss = 0\n",
    "  for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    hidden = hidden.detach()\n",
    "    logits, hidden = model(x_batch, hidden)\n",
    "    loss = criterion(logits, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch: {epoch}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32b877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated_text:  But the caravan began to move, and it was impossible to hear what the Englishman was saying.”\n",
      "I’ve\n",
      "baught, and, where the oasuible the old king\n",
      "the starce how most\n",
      "to explopreads, in the possi\n"
     ]
    }
   ],
   "source": [
    "# Text generate\n",
    "def generated_text(model, start_text, length):\n",
    "  model.eval()\n",
    "  chars = [ch for ch in start_text]\n",
    "  input_vector = torch.tensor([char2int[ch] for ch in chars], dtype=torch.long).unsqueeze(0).to(device)\n",
    "  hidden = model.init_hidden(1).to(device)\n",
    "  with torch.no_grad():\n",
    "    for i in range(len(start_text)-1):\n",
    "      logits, hidden = model(input_vector[:, i:i+1], hidden)\n",
    "    last_char = input_vector[:, -1]\n",
    "\n",
    "\n",
    "    for i in range(length):\n",
    "      logits, hidden = model(last_char.unsqueeze(0), hidden) # logits-> (batch, seq, vocab_size)\n",
    "      probs = torch.softmax(logits.squeeze(), dim=0)\n",
    "      probs = probs.cpu().numpy()\n",
    "      next_idx = np.random.choice(len(probs), p=probs)\n",
    "      # next_idx = torch.argmax(probs).item()\n",
    "\n",
    "      next_char = int2char[next_idx]\n",
    "      chars.append(next_char)\n",
    "      last_char = torch.tensor([next_idx], dtype=torch.long).to(device)\n",
    "\n",
    "  return ''.join(chars)\n",
    "\n",
    "\n",
    "text = 'But the caravan began to move, and it was impossible to hear what the Englishman was saying.'\n",
    "generated = generated_text(model, text, 100)\n",
    "print(\"Generated_text: \", generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
